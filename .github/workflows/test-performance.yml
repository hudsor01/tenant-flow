name: Performance Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
    types: [opened, synchronize, labeled]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'  
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance tests to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - api-only
          - frontend-only
          - database-only
          - load-test
      duration_minutes:
        description: 'Test duration in minutes'
        required: false
        default: '5'
        type: string
      concurrent_users:
        description: 'Number of concurrent users for load testing'
        required: false
        default: '50'
        type: string

env:
  NODE_VERSION: '22.x'
  POSTGRES_VERSION: '15'
  REDIS_VERSION: '7'
  TURBO_TOKEN: ${{ secrets.TURBO_TOKEN }}
  TURBO_TEAM: ${{ vars.TURBO_TEAM }}

concurrency:
  group: performance-tests-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  should-run-performance:
    name: Should Run Performance Tests
    runs-on: ubuntu-latest
    outputs:
      should-run: ${{ steps.decision.outputs.should-run }}
      test-types: ${{ steps.decision.outputs.test-types }}
      is-baseline: ${{ steps.decision.outputs.is-baseline }}
    steps:
      - name: Determine test scope
        id: decision
        run: |
          should_run=false
          test_types="all"
          is_baseline=false

          # Always run on main branch pushes, releases, or schedule
          if [[ "${{ github.ref }}" == "refs/heads/main" ]] || \
             [[ "${{ github.event_name }}" == "schedule" ]] || \
             [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            should_run=true
            is_baseline=true
          fi

          # Run on PRs with performance label
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            if [[ "${{ contains(github.event.pull_request.labels.*.name, 'performance') }}" == "true" ]] || \
               [[ "${{ contains(github.event.pull_request.labels.*.name, 'perf') }}" == "true" ]]; then
              should_run=true
            fi
          fi

          # Use manual inputs if provided
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            test_types="${{ github.event.inputs.test_type }}"
          fi

          echo "should-run=$should_run" >> $GITHUB_OUTPUT
          echo "test-types=$test_types" >> $GITHUB_OUTPUT
          echo "is-baseline=$is_baseline" >> $GITHUB_OUTPUT

  setup-performance-environment:
    name: Setup Performance Environment
    runs-on: ubuntu-latest
    needs: should-run-performance
    if: needs.should-run-performance.outputs.should-run == 'true'
    outputs:
      baseline-data: ${{ steps.baseline.outputs.data }}
    services:
      postgres:
        image: postgres:${{ env.POSTGRES_VERSION }}
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: tenantflow_perf
          POSTGRES_HOST_AUTH_METHOD: trust
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          --shared-buffers=256MB
          --max_connections=200
        ports:
          - 5432:5432

      redis:
        image: redis:${{ env.REDIS_VERSION }}-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Cache Turbo
        uses: actions/cache@v4
        with:
          path: .turbo
          key: ${{ runner.os }}-turbo-performance-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-turbo-performance-
            ${{ runner.os }}-turbo-

      - name: Install dependencies
        run: |
          npm ci --prefer-offline --no-audit --progress=false
          cd apps/backend && npm run generate

      - name: Build applications for performance testing
        run: |
          # Build with production optimizations
          NODE_ENV=production npm run build
        env:
          NODE_ENV: production
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/tenantflow_perf
          VITE_BACKEND_URL: http://localhost:8000
          VITE_API_BASE_URL: http://localhost:8000/api

      - name: Setup performance database
        run: |
          cd apps/backend
          
          # Wait for services
          timeout 60 bash -c 'until pg_isready -h localhost -p 5432 -U postgres; do sleep 1; done'
          timeout 60 bash -c 'until redis-cli -h localhost -p 6379 ping | grep PONG; do sleep 1; done'
          
          # Setup database with optimized settings
          npx prisma migrate deploy --schema=./prisma/schema.prisma
          npx prisma db push --schema=./prisma/schema.prisma --accept-data-loss
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/tenantflow_perf
          NODE_ENV: production

      - name: Seed performance test data
        run: |
          cd apps/backend
          
          # Create large dataset for performance testing
          if [ -f "../tests/seed-performance-data.ts" ]; then
            npx tsx ../tests/seed-performance-data.ts
          else
            # Create basic performance test data
            cat << 'EOF' > ../tests/temp-perf-seed.ts
            import { PrismaClient } from '@prisma/client';
            
            const prisma = new PrismaClient();
            
            async function seedPerformanceData() {
              console.log('Seeding performance test data...');
              
              // Create test organizations (50)
              const orgs = [];
              for (let i = 1; i <= 50; i++) {
                orgs.push({
                  name: `Performance Org ${i}`,
                  slug: `perf-org-${i}`,
                });
              }
              
              await prisma.organization.createMany({ data: orgs });
              console.log('Created 50 organizations');
              
              // Create properties for each org (5 per org = 250 total)
              const orgIds = await prisma.organization.findMany({ select: { id: true } });
              
              for (const org of orgIds) {
                const properties = [];
                for (let i = 1; i <= 5; i++) {
                  properties.push({
                    name: `Property ${i} - Org ${org.id}`,
                    address: `${i}00 Test St, Performance City, PC 12345`,
                    organizationId: org.id,
                  });
                }
                await prisma.property.createMany({ data: properties });
              }
              
              console.log('Created 250 properties');
              console.log('Performance test data seeding completed');
            }
            
            seedPerformanceData().catch(console.error).finally(() => prisma.$disconnect());
            EOF
            
            npx tsx ../tests/temp-perf-seed.ts
            rm ../tests/temp-perf-seed.ts
          fi
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/tenantflow_perf
          NODE_ENV: test

      - name: Start services for performance testing
        run: |
          # Start backend with production settings  
          cd apps/backend
          NODE_ENV=production npm run start:prod &
          BACKEND_PID=$!
          echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV
          
          # Start frontend
          cd ../frontend
          npm run preview -- --port 3000 --host &
          FRONTEND_PID=$!
          echo "FRONTEND_PID=$FRONTEND_PID" >> $GITHUB_ENV
          
          # Wait for services to be ready
          timeout 120 bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'
          timeout 60 bash -c 'until curl -f http://localhost:3000; do sleep 2; done'
          
          echo "Services are ready for performance testing"
        env:
          NODE_ENV: production
          PORT: 8000
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/tenantflow_perf
          REDIS_URL: redis://localhost:6379
          JWT_SECRET: performance-test-secret

      - name: Load baseline performance data
        id: baseline
        if: needs.should-run-performance.outputs.is-baseline == 'false'
        run: |
          # Try to download previous baseline from cache or artifacts
          if [ -f "performance-baseline.json" ]; then
            echo "data=$(cat performance-baseline.json | jq -c .)" >> $GITHUB_OUTPUT
          else
            echo "data={}" >> $GITHUB_OUTPUT
          fi

      - name: Cache performance environment
        uses: actions/cache@v4
        with:
          path: |
            performance-baseline.json
            performance-environment-info.json
          key: ${{ runner.os }}-performance-env-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-performance-env-

  api-performance-tests:
    name: API Performance Tests
    runs-on: ubuntu-latest
    needs: [should-run-performance, setup-performance-environment]
    if: needs.should-run-performance.outputs.should-run == 'true' && (needs.should-run-performance.outputs.test-types == 'all' || needs.should-run-performance.outputs.test-types == 'api-only')
    timeout-minutes: 20
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install performance testing tools
        run: |
          npm install -g autocannon k6
          npm ci --prefer-offline --no-audit --progress=false

      - name: Run API endpoint performance tests
        run: |
          duration="${{ github.event.inputs.duration_minutes || '5' }}"
          
          echo "Running API performance tests for ${duration} minutes..."
          
          # Test key endpoints with autocannon
          echo "Testing /health endpoint..."
          autocannon -c 10 -d ${duration}m --json http://localhost:8000/health > health-perf.json
          
          echo "Testing /api/properties endpoint..."
          autocannon -c 20 -d ${duration}m --json \
            -H "Authorization: Bearer test-token" \
            http://localhost:8000/api/properties > properties-perf.json
          
          echo "Testing /api/users/profile endpoint..."
          autocannon -c 15 -d ${duration}m --json \
            -H "Authorization: Bearer test-token" \
            http://localhost:8000/api/users/profile > profile-perf.json

      - name: Run database query performance tests
        if: needs.should-run-performance.outputs.test-types == 'all' || needs.should-run-performance.outputs.test-types == 'database-only'
        run: |
          cd apps/backend
          
          # Create database performance test
          cat << 'EOF' > db-perf-test.js
          const { PrismaClient } = require('@prisma/client');
          const prisma = new PrismaClient();
          
          async function testDatabasePerformance() {
            console.log('Testing database query performance...');
            
            const start = Date.now();
            
            // Test various query patterns
            const tests = [
              { name: 'Simple select', fn: () => prisma.organization.findMany({ take: 10 }) },
              { name: 'Complex join', fn: () => prisma.property.findMany({ 
                include: { organization: true, units: true }, 
                take: 10 
              }) },
              { name: 'Aggregation', fn: () => prisma.property.count() },
            ];
            
            const results = {};
            
            for (const test of tests) {
              const iterations = 100;
              const startTime = Date.now();
              
              for (let i = 0; i < iterations; i++) {
                await test.fn();
              }
              
              const endTime = Date.now();
              const avgTime = (endTime - startTime) / iterations;
              
              results[test.name] = {
                avgResponseTime: avgTime,
                totalIterations: iterations,
                totalTime: endTime - startTime
              };
              
              console.log(`${test.name}: ${avgTime.toFixed(2)}ms average`);
            }
            
            console.log(JSON.stringify(results, null, 2));
          }
          
          testDatabasePerformance().catch(console.error).finally(() => prisma.$disconnect());
          EOF
          
          node db-perf-test.js > ../db-performance-results.json
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/tenantflow_perf

      - name: Analyze API performance results
        run: |
          # Parse autocannon results and create summary
          cat << 'EOF' > analyze-perf.js
          const fs = require('fs');
          
          function analyzeResults() {
            const results = {};
            
            const files = ['health-perf.json', 'properties-perf.json', 'profile-perf.json'];
            
            files.forEach(file => {
              if (fs.existsSync(file)) {
                const data = JSON.parse(fs.readFileSync(file, 'utf8'));
                const endpoint = file.replace('-perf.json', '');
                
                results[endpoint] = {
                  requests: data.requests?.total || 0,
                  throughput: data.throughput?.average || 0,
                  latency: {
                    average: data.latency?.average || 0,
                    p95: data.latency?.p95 || 0,
                    p99: data.latency?.p99 || 0
                  },
                  errors: data.errors || 0
                };
              }
            });
            
            fs.writeFileSync('api-performance-summary.json', JSON.stringify(results, null, 2));
            console.log('API Performance Summary:');
            console.log(JSON.stringify(results, null, 2));
          }
          
          analyzeResults();
          EOF
          
          node analyze-perf.js

      - name: Upload API performance results
        uses: actions/upload-artifact@v4
        with:
          name: api-performance-results
          path: |
            *-perf.json
            api-performance-summary.json
            db-performance-results.json
          retention-days: 14

  frontend-performance-tests:
    name: Frontend Performance Tests
    runs-on: ubuntu-latest
    needs: [should-run-performance, setup-performance-environment]
    if: needs.should-run-performance.outputs.should-run == 'true' && (needs.should-run-performance.outputs.test-types == 'all' || needs.should-run-performance.outputs.test-types == 'frontend-only')
    timeout-minutes: 15
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci --prefer-offline --no-audit --progress=false

      - name: Install Playwright and Lighthouse
        run: |
          npx playwright install chromium --with-deps
          npm install -g @lhci/cli

      - name: Run Lighthouse performance audit
        run: |
          # Run Lighthouse on key pages
          lhci autorun \
            --upload.target=temporary-public-storage \
            --collect.numberOfRuns=3 \
            --collect.url="http://localhost:3000" \
            --collect.url="http://localhost:3000/dashboard" \
            --collect.url="http://localhost:3000/properties" \
            --assert.preset="lighthouse:recommended" \
            --budgets-file=.lighthouserc.json || echo "Lighthouse completed with warnings"

      - name: Run Core Web Vitals tests
        run: |
          # Create Playwright test for Core Web Vitals
          cat << 'EOF' > web-vitals-test.js
          const { chromium } = require('playwright');
          
          async function measureWebVitals() {
            const browser = await chromium.launch();
            const context = await browser.newContext();
            const page = await context.newPage();
            
            const vitals = {};
            
            // Measure Core Web Vitals
            await page.addInitScript(() => {
              new PerformanceObserver((list) => {
                for (const entry of list.getEntries()) {
                  if (entry.entryType === 'navigation') {
                    window.navigationTiming = entry;
                  }
                }
              }).observe({ entryTypes: ['navigation'] });
            });
            
            const pages = [
              { name: 'home', url: 'http://localhost:3000' },
              { name: 'dashboard', url: 'http://localhost:3000/dashboard' },
              { name: 'properties', url: 'http://localhost:3000/properties' }
            ];
            
            for (const testPage of pages) {
              await page.goto(testPage.url, { waitUntil: 'networkidle' });
              
              const metrics = await page.evaluate(() => {
                const perf = performance.getEntriesByType('navigation')[0];
                return {
                  domContentLoaded: perf.domContentLoadedEventEnd - perf.domContentLoadedEventStart,
                  loadComplete: perf.loadEventEnd - perf.loadEventStart,
                  firstPaint: performance.getEntriesByType('paint').find(e => e.name === 'first-paint')?.startTime || 0,
                  firstContentfulPaint: performance.getEntriesByType('paint').find(e => e.name === 'first-contentful-paint')?.startTime || 0
                };
              });
              
              vitals[testPage.name] = metrics;
              console.log(`${testPage.name}:`, metrics);
            }
            
            await browser.close();
            
            require('fs').writeFileSync('web-vitals-results.json', JSON.stringify(vitals, null, 2));
          }
          
          measureWebVitals().catch(console.error);
          EOF
          
          node web-vitals-test.js

      - name: Upload frontend performance results
        uses: actions/upload-artifact@v4
        with:
          name: frontend-performance-results
          path: |
            .lighthouseci/
            web-vitals-results.json
          retention-days: 14

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    needs: [should-run-performance, setup-performance-environment]
    if: needs.should-run-performance.outputs.should-run == 'true' && (needs.should-run-performance.outputs.test-types == 'all' || needs.should-run-performance.outputs.test-types == 'load-test')
    timeout-minutes: 25
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install k6 for load testing
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Create load test script
        run: |
          concurrent_users="${{ github.event.inputs.concurrent_users || '50' }}"
          duration="${{ github.event.inputs.duration_minutes || '5' }}"
          
          cat << EOF > load-test.js
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate } from 'k6/metrics';
          
          export let options = {
            stages: [
              { duration: '1m', target: Math.floor(${concurrent_users} * 0.3) }, // Ramp up
              { duration: '${duration}m', target: ${concurrent_users} }, // Stay at load
              { duration: '1m', target: 0 }, // Ramp down
            ],
            thresholds: {
              http_req_duration: ['p(95)<500'], // 95% of requests must complete below 500ms
              http_req_failed: ['rate<0.1'], // Error rate must be below 10%
            },
          };
          
          export default function () {
            // Test different endpoints with realistic user behavior
            let responses = http.batch([
              ['GET', 'http://localhost:8000/health'],
              ['GET', 'http://localhost:8000/api/properties', null, {
                headers: { 'Authorization': 'Bearer test-token' }
              }],
              ['GET', 'http://localhost:3000/'],
            ]);
            
            check(responses[0], {
              'health check status is 200': (r) => r.status === 200,
            });
            
            check(responses[1], {
              'properties API status is 200 or 401': (r) => r.status === 200 || r.status === 401,
            });
            
            check(responses[2], {
              'frontend status is 200': (r) => r.status === 200,
            });
            
            sleep(1); // Simulate user think time
          }
          EOF

      - name: Run load test
        run: |
          k6 run --out json=load-test-results.json load-test.js

      - name: Analyze load test results
        run: |
          # Extract key metrics from k6 results
          cat << 'EOF' > analyze-load-test.js
          const fs = require('fs');
          
          const results = fs.readFileSync('load-test-results.json', 'utf8')
            .split('\n')
            .filter(line => line.trim())
            .map(line => JSON.parse(line))
            .filter(item => item.type === 'Point');
          
          const summary = {
            http_req_duration: {
              avg: 0,
              p95: 0,
              max: 0
            },
            http_req_failed: 0,
            http_reqs: 0,
            vus: 0
          };
          
          // Process metrics
          results.forEach(point => {
            if (point.metric === 'http_req_duration') {
              summary.http_req_duration.avg = point.data.value;
            } else if (point.metric === 'http_req_failed') {
              summary.http_req_failed = point.data.value;
            } else if (point.metric === 'http_reqs') {
              summary.http_reqs = point.data.value;
            } else if (point.metric === 'vus') {
              summary.vus = Math.max(summary.vus, point.data.value);
            }
          });
          
          fs.writeFileSync('load-test-summary.json', JSON.stringify(summary, null, 2));
          console.log('Load Test Summary:', summary);
          EOF
          
          node analyze-load-test.js

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: |
            load-test-results.json
            load-test-summary.json
          retention-days: 14

  performance-regression-analysis:
    name: Performance Regression Analysis
    runs-on: ubuntu-latest
    needs: [should-run-performance, api-performance-tests, frontend-performance-tests, load-testing]
    if: always() && needs.should-run-performance.outputs.should-run == 'true'
    steps:
      - name: Download all performance results
        uses: actions/download-artifact@v4
        with:
          pattern: "*-performance-results"
          merge-multiple: true

      - name: Download load test results
        uses: actions/download-artifact@v4
        with:
          name: load-test-results

      - name: Analyze performance regression
        run: |
          # Create comprehensive performance analysis
          cat << 'EOF' > performance-analysis.js
          const fs = require('fs');
          
          function analyzePerformance() {
            const report = {
              timestamp: new Date().toISOString(),
              summary: {
                overall_status: 'PASS',
                critical_issues: [],
                warnings: [],
                improvements: []
              },
              api_performance: {},
              frontend_performance: {},
              load_testing: {}
            };
            
            // Analyze API performance
            if (fs.existsSync('api-performance-summary.json')) {
              const apiData = JSON.parse(fs.readFileSync('api-performance-summary.json', 'utf8'));
              report.api_performance = apiData;
              
              // Check for performance issues
              Object.entries(apiData).forEach(([endpoint, metrics]) => {
                if (metrics.latency.p95 > 1000) {
                  report.summary.critical_issues.push(`${endpoint}: P95 latency ${metrics.latency.p95}ms exceeds 1000ms threshold`);
                  report.summary.overall_status = 'FAIL';
                } else if (metrics.latency.p95 > 500) {
                  report.summary.warnings.push(`${endpoint}: P95 latency ${metrics.latency.p95}ms above 500ms`);
                }
                
                if (metrics.errors > 0) {
                  report.summary.critical_issues.push(`${endpoint}: ${metrics.errors} errors detected`);
                  report.summary.overall_status = 'FAIL';
                }
              });
            }
            
            // Analyze frontend performance
            if (fs.existsSync('web-vitals-results.json')) {
              const frontendData = JSON.parse(fs.readFileSync('web-vitals-results.json', 'utf8'));
              report.frontend_performance = frontendData;
              
              Object.entries(frontendData).forEach(([page, metrics]) => {
                if (metrics.firstContentfulPaint > 2500) {
                  report.summary.warnings.push(`${page}: First Contentful Paint ${metrics.firstContentfulPaint}ms is slow`);
                }
                
                if (metrics.domContentLoaded > 1500) {
                  report.summary.warnings.push(`${page}: DOM Content Loaded ${metrics.domContentLoaded}ms is slow`);
                }
              });
            }
            
            // Analyze load testing
            if (fs.existsSync('load-test-summary.json')) {
              const loadData = JSON.parse(fs.readFileSync('load-test-summary.json', 'utf8'));
              report.load_testing = loadData;
              
              if (loadData.http_req_failed > 0.05) { // 5% error rate threshold
                report.summary.critical_issues.push(`Load test error rate ${(loadData.http_req_failed * 100).toFixed(2)}% exceeds 5% threshold`);
                report.summary.overall_status = 'FAIL';
              }
              
              if (loadData.http_req_duration.avg > 300) {
                report.summary.warnings.push(`Load test average response time ${loadData.http_req_duration.avg}ms above 300ms`);
              }
            }
            
            fs.writeFileSync('performance-analysis-report.json', JSON.stringify(report, null, 2));
            return report;
          }
          
          const report = analyzePerformance();
          console.log('Performance Analysis Report:');
          console.log(JSON.stringify(report.summary, null, 2));
          EOF
          
          node performance-analysis.js

      - name: Generate performance summary
        run: |
          echo "# Performance Test Results 🚀" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "performance-analysis-report.json" ]; then
            status=$(cat performance-analysis-report.json | jq -r '.summary.overall_status')
            
            if [[ "$status" == "PASS" ]]; then
              echo "✅ **Overall Status**: All performance tests passed" >> $GITHUB_STEP_SUMMARY
            else
              echo "❌ **Overall Status**: Performance issues detected" >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Test Results 📊" >> $GITHUB_STEP_SUMMARY
            
            # Add API performance results
            echo "### API Performance" >> $GITHUB_STEP_SUMMARY
            if [ -f "api-performance-summary.json" ]; then
              echo "- Health endpoint tested ✅" >> $GITHUB_STEP_SUMMARY
              echo "- Properties API tested ✅" >> $GITHUB_STEP_SUMMARY
              echo "- User profile API tested ✅" >> $GITHUB_STEP_SUMMARY
            else
              echo "- No API performance data available" >> $GITHUB_STEP_SUMMARY
            fi
            
            # Add frontend performance results
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Frontend Performance" >> $GITHUB_STEP_SUMMARY
            if [ -f "web-vitals-results.json" ]; then
              echo "- Core Web Vitals measured ✅" >> $GITHUB_STEP_SUMMARY
              echo "- Multiple page performance tested ✅" >> $GITHUB_STEP_SUMMARY
            else
              echo "- No frontend performance data available" >> $GITHUB_STEP_SUMMARY
            fi
            
            # Add load testing results
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Load Testing" >> $GITHUB_STEP_SUMMARY
            if [ -f "load-test-summary.json" ]; then
              concurrent_users="${{ github.event.inputs.concurrent_users || '50' }}"
              echo "- Concurrent users: $concurrent_users ✅" >> $GITHUB_STEP_SUMMARY
              echo "- System stability under load tested ✅" >> $GITHUB_STEP_SUMMARY
            else
              echo "- No load testing data available" >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Performance Thresholds 🎯" >> $GITHUB_STEP_SUMMARY
            echo "- **API Response Time**: P95 < 500ms" >> $GITHUB_STEP_SUMMARY
            echo "- **Frontend FCP**: < 2.5s" >> $GITHUB_STEP_SUMMARY
            echo "- **Error Rate**: < 5%" >> $GITHUB_STEP_SUMMARY
            echo "- **Availability**: > 99%" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload performance analysis
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis
          path: |
            performance-analysis-report.json
          retention-days: 30

      - name: Save baseline performance data
        if: needs.should-run-performance.outputs.is-baseline == 'true'
        run: |
          # Save current performance data as baseline for future comparisons
          if [ -f "performance-analysis-report.json" ]; then
            cp performance-analysis-report.json performance-baseline.json
          fi

      - name: Comment on PR with performance results
        if: github.event_name == 'pull_request' && needs.should-run-performance.outputs.is-baseline == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            if (!fs.existsSync('performance-analysis-report.json')) {
              console.log('No performance analysis report found');
              return;
            }
            
            const report = JSON.parse(fs.readFileSync('performance-analysis-report.json', 'utf8'));
            const status = report.summary.overall_status === 'PASS' ? '✅' : '❌';
            
            let body = `## ${status} Performance Test Results\n\n`;
            
            if (report.summary.critical_issues.length > 0) {
              body += `### ❌ Critical Issues\n`;
              report.summary.critical_issues.forEach(issue => {
                body += `- ${issue}\n`;
              });
              body += '\n';
            }
            
            if (report.summary.warnings.length > 0) {
              body += `### ⚠️ Warnings\n`;
              report.summary.warnings.forEach(warning => {
                body += `- ${warning}\n`;
              });
              body += '\n';
            }
            
            body += `### 📊 Performance Summary\n`;
            body += `- **Overall Status**: ${report.summary.overall_status}\n`;
            body += `- **Critical Issues**: ${report.summary.critical_issues.length}\n`;
            body += `- **Warnings**: ${report.summary.warnings.length}\n`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body
            });

  cleanup-performance-environment:
    name: Cleanup Performance Environment
    runs-on: ubuntu-latest
    needs: [api-performance-tests, frontend-performance-tests, load-testing]
    if: always()
    steps:
      - name: Cleanup performance test resources
        run: |
          echo "Cleaning up performance test environment..."
          # Kill any remaining processes
          pkill -f "node\|npm\|autocannon\|k6" || true
          echo "Performance test cleanup completed"